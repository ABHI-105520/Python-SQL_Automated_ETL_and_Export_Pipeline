{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python-SQL Automated ETL and Export Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Import libraries.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• pyodbc for connecting to database.<br>• pandas to handle dataframe.<br>• csv to handle data export from database to .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Extract data from csv*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pandas, reading csv file and extracting data to a dataframe. Checking the first 3 rows of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"employees$.csv\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Transform Data in Dataframe*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our dataframe is ready, we can clean and transform the data for further process. So we're checking some info of our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there are no null values. But something is suspecious here. So we will check the values of major columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"COMMISSION PCT\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this column contains only '-' and no actual value that specifies the column name. This column is useless for now, so will drop it.<br>If required we'll create it when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(\"COMMISSION PCT\", axis=1)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done. we'll check the same for some other columns too untill we find any non-null dummy value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"EMPLOYEE ID\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DEPARTMENT ID\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"MANAGER ID\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see there is as unusual data that doesn't match with the data present in the column. We need to handle this.<br>So lets go down to the logic here. The manager id is missing for a department. What we can do is,<br>• Check the dept id of the missing manager id.<br>• Find the same dept id where manager id is mentioned.<br>• Fill the missing manager id with the id fetched by comparing dept id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"MANAGER ID\"]=df[\"MANAGER ID\"].astype(str)\n",
    "mis=df[df[\"MANAGER ID\"]==\" - \"].index\n",
    "\n",
    "for i in mis:\n",
    "    dept=df.loc[i,\"DEPARTMENT ID\"]\n",
    "    mng=df.loc[(df[\"DEPARTMENT ID\"]==dept) & (df[\"MANAGER ID\"]!=\" - \"),\"MANAGER ID\"]\n",
    "    if not mng.empty:\n",
    "        df.loc[i,\"MANAGER ID\"]=mng.values[0]\n",
    "\n",
    "df[\"MANAGER ID\"]=df[\"MANAGER ID\"].astype(\"int64\")\n",
    "df[\"MANAGER ID\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll clean the file name for our table name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_nm=\"employees$\"\n",
    "cl_tbl_nm=f_nm.replace(\"$\",\"\")\n",
    "cl_tbl_nm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same we'll do to the column names for our table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=[x.lower().replace(\" \",\"_\") for x in df.columns]\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll check the data type of our columns in dataframe as we need to replace it with data type for our SQL server while executing our query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here, we're providing alternate data types for our SQL query, joining it with column names and making it ready to copy for our query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typcnvr={\n",
    "    'int64':'int',\n",
    "    'object':'varchar'\n",
    "}\n",
    "cl_nm=', '.join(\"{} {}\".format(n, d) for (n ,d) in zip(df.columns, df.dtypes.replace(typcnvr)))\n",
    "cl_nm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our data is ready to be loaded to database.<br>We'll first export our dataframe to csv file so that we can read and pass it to our query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('employee.csv', header=df.columns, index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Connect py script to database.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I'm connecting to SQL Server using windows authentication. Establishing a cursor that'll pass our queries to server. Using try and except method to fetch error(if any) and display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    conn=pyodbc.connect(\n",
    "        'Driver={Driver Name};'\n",
    "        'Server=Server Name;'\n",
    "        'Database=Database Name;'\n",
    "        'Trusted_Connection=Yes;'\n",
    "    )\n",
    "    print(\"Server Connected Successfully! opened database\")\n",
    "    cursor=conn.cursor()\n",
    "    print(\"Cursor ready!\")\n",
    "except pyodbc.Error as e:\n",
    "    print(\"Error in establishing connection: \",e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using cursor, we'll pass a query to create a table. Here we can copy the standardized table name from cl_tbl_nm and column name from cl_nm. After which we'll commit the query to our connected SQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE employees(\n",
    "            employee_id int,\n",
    "            first_name varchar(20),\n",
    "            last_name varchar(20),\n",
    "            email varchar(30),\n",
    "            phone_number varchar(15),\n",
    "            hire_date varchar(10),\n",
    "            job_id varchar(10),\n",
    "            salary int,\n",
    "            manager_id int,\n",
    "            department_id int\n",
    "        );\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    print(\"Table created successfully!\")\n",
    "except pyodbc.Error as e:\n",
    "    print(\"Error in creating table: \", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Load data to Database.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we created our desired table, we can load our data to database. The data we transformed and saved in csv, we'll<br>• Open that file<br>• Read each line, split as per delimiter ',' and create a list of those data<br>• Load each item of the list as per columns via insert query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    l=0\n",
    "    with open('employee.csv') as d_file:\n",
    "        next(d_file)    #skip the first row, containinig headers.\n",
    "        for line in d_file:\n",
    "            values=line.strip().split(',')\n",
    "            cursor.execute(\"INSERT INTO employees VALUES(?,?,?,?,?,?,?,?,?,?);\",values)\n",
    "            l+=1    #counting no. of rows inserted\n",
    "    conn.commit()\n",
    "    print(f\"Data loaded to db with {l} rows!\")\n",
    "except pyodbc.Error as e:\n",
    "    print(\"Error in copying data from csv to db: \",e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After inserting data, let's look into our table. Tabulate is used to display the fetched data in tabular format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cursor.execute(\"SELECT TOP 5 * FROM employees;\")\n",
    "    print(tabulate(cursor.fetchall(), headers=[c[0] for c in cursor.description], tablefmt='grid'))\n",
    "except pyodbc.Error as e:\n",
    "    print(\"Error: \", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us add some more data in the database with query passed to database via cursor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cursor.execute(\"\"\"INSERT INTO employees VALUES\n",
    "                    (141, 'Susan', 'Mavrick', 'SMAVRICK', '515.123.7777', '10-Oct-06', 'MK_REP', 6200, 201, 20),\n",
    "                    (142, 'Chris', 'Taylor', 'CTAYLOR', '650.507.8888', '01-Mar-07', 'SH_CLERK', 2700, 124, 50),\n",
    "                    (143, 'Nancy', 'Jones', 'NJONES', '515.123.9999', '12-Dec-02', 'AD_ASST', 4500, 101, 10),\n",
    "                    (144, 'Matthew', 'Smith', 'MSMITH', '603.123.1234', '22-May-09', 'IT_PROG', 9200, 102, 60),\n",
    "                    (145, 'Emily', 'Brown', 'EBROWN', '515.123.4321', '15-Jul-10', 'IT_PROG', 8700, 102, 60),\n",
    "                    (146, 'Robert', 'White', 'RWHITE', '650.507.5678', '19-Jan-12', 'ST_CLERK', 3100, 203, 50),\n",
    "                    (147, 'Sophia', 'Green', 'SGREEN', '603.123.6543', '23-Jun-08', 'ST_CLERK', 2800, 203, 50),\n",
    "                    (148, 'William', 'Black', 'WBLACK', '515.123.8888', '30-Apr-11', 'HR_REP', 8000, 200, 40),\n",
    "                    (149, 'Linda', 'Morris', 'LMORRIS', '650.507.2468', '10-Nov-13', 'HR_REP', 7500, 200, 40),\n",
    "                    (150, 'James', 'Wilson', 'JWILSON', '603.123.7890', '27-Aug-14', 'SH_CLERK', 2900, 124, 50);                   \n",
    "                   \"\"\")\n",
    "    print(\"Data inserted!\")\n",
    "except pyodbc.Error as e:\n",
    "    print(\"Error: \", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Exporting Data from database.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First method is exporting data from database directly to csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch required data with select query and store in a variable(rows). Fetch columns and store in a variable(columns). Create and open a csv file and create a writer object to write data to csv file. Load column name and rows accordingly into the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cursor.execute(\"SELECT * FROM employees\")\n",
    "    rows=cursor.fetchall()\n",
    "    columns=[c[0] for c in cursor.description]\n",
    "    with open('emp_dr_csv.csv',mode='w+',newline='',encoding='utf-8') as file:\n",
    "        data=csv.writer(file)\n",
    "        data.writerow(columns)\n",
    "        data.writerows(rows)\n",
    "    print(\"Data exported from database to csv file!\")\n",
    "\n",
    "except pyodbc.Error as e:\n",
    "    print(\"Error: \",e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Second method is by exporting data from database to dataframe and then to required file type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll read the output of the SQL query and load it to a dataframe using pandas.<br>This method let us perform further transformations and analysis before exporting to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df=pd.read_sql(\"SELECT * FROM employees\", conn)\n",
    "    print(\"Data loaded from database to dataframe!\")\n",
    "except pyodbc.Error as e:\n",
    "    print(\"Error: \",e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the data is loaded to dataframe, we can perform various analysis as per requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform some simple EDA on our data.<br>Here I want to know about the Name, Job id and salary of the employee who gets the maximum and minimum salaries in the team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_mx=df.loc[df['salary']==df['salary'].max()]\n",
    "print(f\"{l_mx['first_name'].values[0].upper()} {l_mx['last_name'].values[0].upper()} with job id '{l_mx['job_id'].values[0]}' gets the highest salary ${l_mx['salary'].values[0]}\")\n",
    "l_mn=df.loc[df['salary']==df['salary'].min()]\n",
    "print(f\"{l_mn['first_name'].values[0].upper()} {l_mn['last_name'].values[0].upper()} with job id '{l_mn['job_id'].values[0]}' gets the lowest salary ${l_mn['salary'].values[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise we can perform further EDA, data manipulation, visualizations etc. before exporting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After which, we are ready to export the data, that we extracted from database, from dataframe to to desired file type.<br>So first we'll export to csv format with proper encoding. Provide desired file name, a csv file will be created in the directory containing extracted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('emp_df_csv.csv',index=False,encoding='utf-8')\n",
    "print(\"Data exported from dataframe to csv file!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, we can export the data to excel file from dataframe. Provide desired file name, a xlsx file will be created in the directory containing extracted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('emp_df_xlsx.xlsx',index=False,sheet_name='Sheet1')\n",
    "print(\"Data exported from dataframe to xlsx file!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Close connection between py script and database.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close the connection any time when task is completed and there are no more requirements of the connection to database. If the connection remains open and any query is passed to the database by mistake, it'll hamper the data in database.<br>Closing connection immediate to end of task with database is suggested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
